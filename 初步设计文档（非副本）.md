- ## 系统架构

  整体架构采用分布式的方式，包括Coordinator、Worker和Aggregator三个主要组件。

  1. **Coordinator（协调节点）：** 负责任务的分配和调度，管理整个系统的状态信息。
  2. **Worker（工作节点）：** 负责更新Git仓库、处理数据。可部署在各个家庭服务器上，无需公网IP。
  3. **Aggregator（聚合节点）：** 负责汇总各个Worker数据，从worker上拉取更新，并提供统一的服务接口给外部用户。Aggregator拥有公网IP，可供外部访问。

  

  ## 组件功能

  1. **Coordinator：**

     - 轻量级节点，拥有公网IP
     - 读取配置文件，管理数据。
     - 负责任务的分配和调度，将Git仓库URL分配给不同的Worker。
     - 与Aggregator交互。

     需要保证Coordinator的轻量级-> 

     1、高效任务调度算法+状态管理（避免不必要的计算和存储开销）

     2、需要保证其设计和功能的简化

     3、异步和并发处理

  2. **Worker：**

     - 从Git托管服务商（如GitHub、GitLab、Gitee）更新Git仓库数据，并实现增量同步。
     - 提供服务接口，响应Aggregator和Coordinator的请求。
     - 将本地状态和数据汇报给Coordinator。

  3. **Aggregator：**

     - 从各个Worker收集数据，并进行汇总。
     - 拥有公网IP，提供统一的服务接口给外部用户，例如提交任务、获取数据等。

  

  > **【worker/aggregator是否可合并? 成为一个模块里的两种运行模式：partial/full模式】**
  >
  > 可行。
  >
  > **Partial模式：** 在Partial模式下充当Worker的角色，负责从上游Git托管服务拉取指定的Git仓库数据并进行增量同步。它会定期轮询上游仓库，根据更新活跃度自适应地执行Git Pull操作频率。此模式下的核心功能是实现Git仓库的增量同步。
  >
  > **Full模式：** 在Full模式下充当Aggregator的角色，负责整体协调和服务提供。它会接收来自Partial模式的数据，并将所有拉取的仓库数据集中在一起。在Full模式中，还包含处理API请求、动态添加/更新Git URL等功能。此模式下的核心功能是对外提供API接口，允许用户按需触发Git仓库的拉取操作，并管理Git仓库的配置信息。

  

  ## 通信方式

  1. **Coordinator与Worker之间：**

     基于低延迟、高性能要求，可使用轻量级RPC来实现通信。

     Worker向Coordinator注册自己的状态，接收分配的任务，并定期汇报任务执行状态和结果

  2. **Aggregator与Worker之间：** 

     同样可使用RPC来实现通信。

     Aggregator向Worker请求最新数据

     wfg: Aggregator是否可以直接git fetch方式访问Worker? Worker直接起一个本地git server服务即可。
     不过要求Worker与Aggregator之间建立一种代理或者隧道。因为Worker没有公网IP，通常Aggregator无法主动直接访问Worker。

     git push to Aggregator?
     安全性 对公众开放 危险 会被攻击

  3. **Coordinator与Aggregator之间：** 

     这两者之间的通信需要更稳定和可靠的公共接口-> 可使用HTTP协议来实现

     Coordinator将从Worker收集到的数据请求发送给Aggregator，Aggregator进行数据汇总和处理后，将结果返回给Coordinator。

  4. **外部用户与Aggregator之间的通信：** 用户可通过HTTP API与Aggregator交互，提交任务、获取数据等操作。

  

  【?】

    worker<>aggregator<>coordinator
    worker<>coordinator speed 10MB/s
    aggregator<>coordinator speed 100MB/s
  
  
  
  

  ## 仓库下发worker机制【hash机制】
  
  Coordinator中存有仓库URL list，采用一致性哈希算法来分配对应的worker。
  
  由于在分布式集群中worker节点的个数会不定时调整（增加n个worker或n个worker挂机的情况），如果采用常规哈希，在面对节点数量变化时选择重新去仓库列表源Coordinator中大规模进行仓库的重新分配，效率会很低且耗时间。
  
  考虑节点数量变化的场景，可以采用一致性哈希算法去解决。
  
  
  
  将repo哈希值映射到0-n空间中，且将这个范围首尾相连形成一个环。再解决worker负载不均的问题（当worker数量少时，有种情况是如果当前workerA挂了，那么原本workerA维护的n个仓库将全部由顺时针的下个workerB进行维护，导致负载不均），在一致性哈希的基础上引入虚拟worker的概念，一个真实worker对应n个虚拟worker。代价非常小，只需增加一个map维护真实节点与虚拟节点的映射关系即可。
  
  映射过程如下：
  
	Coordinator给出一个URL-> hash-> 对应到对应的worker
	
  hash过程如下：
  
  - 计算虚拟worker(使用worker的IP地址)的哈希值，放在环上。
  - 计算仓库（使用仓库URL）的哈希值，对哈希值进行判断-> 顺时针寻找到的第一个节点，就是应选取对应的worker。
  
  *（可参考图“一致性哈希.png”，其中key即为repo，peer即为worker）*
  
	![](https://github.com/yanyanran/pictures/blob/main/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C.png?raw=true)
	
	【?】插入新worker后，旧worker.next上的仓库是否需要取消其在worker上的维护？ 需要
	
	- insert new worker：
  
  ​	traverse hash repo：new_worker.pre-> new_worker之间
  
  ​		【coordinator修改映射】 (1by1事务) delete hash in new_worker.next => add hash in new_worker
  
  ​		【send to worker修改worker内部维护的repo list】send rpc -> [new_worker]add_repo、[new_worker.next]delete_repo
  
  
  
  【连接好一定量worker后再分发任务 VS 每连接一个worker就hash分发一部分repo（存在**抖动问题**）】
  
  - 方案1：最初启动时先启动好一定量的worker，等全部启动好后再进行分发【？启动100个worker需要多久？（是否影响coordinator“快速启动”的需求）】
  
  
  
  - 方案2：先定环上peer数，并提前算好repo和worker之间的映射【万一指定100个worker但是有worker挂了怎么办】
  
    人为部署worker初始化数量n，coordinator启动，此时哈希环的peer数为n，开始计算repo_list中的仓库对应的hash_worker。
  
    开始监听，每和一个worker连接-> worker_conn++，然后开始通过map查找
  
    记录第一个worker连接的时间first_worker_connTime，之后每次worker连接就用【当前时间time-first_worker_connTime】/ worker_conn=> 得出时间间隔作为等待新连接的超时时长。
  
    如果在超时未有worker连接，则开始分发任务给worker
  
    
  
  关于组件之间的通信=> 统一使用远程过程调用（grpc）：
  
  在worker本地注册处理函数，coordinator负责发送对应请求即可触发worker调用对应函数
  
  【？具体业务函数】
  
  
  
  
  
  ##  POLL interval 自适应算法
  
  可延用git-mirror.rb中的逻辑在各worker中【优先级和优先级队列结合】
  
  同步任务队列**git_queue**<->优先级队列**priority.queue**
  
  worker_main_loop：
  
  ​		priority.queue.pop【从优先队列delete_min】-> 
  
  ​			push_git_queue【放入任务队列】-> 
  
  ​				get_repo_priority【计算新优先级】-> 
  
  ​					priority.queue.push 【放回优先级队列】
  
  new_10_threads：git_queue.pop【从任务队列中取任务进行处理】
  
  ```c
  STEP_SECONDS = 2592000 // 每个步骤的时间间隔
  
  func get_repo_priority(git_repo, old_pri) {
      mirror_dir = find .git file
  	step = (git_repo.clone_fail_cnt + 1) * cbrt(STEP_SECONDS)  // clone_fail
      if(mirror_dir == null) {
          return old_pri + step  // 仓库未克隆
      }
      return cal_priority(mirror_dir, old_dir, git_repo)
  }
  
  func cal_priority(mirror_dir, old_pri, git_repo) {
      last_commit_time = ...
      step = (git_repo.fetch_fail_cnt + 1) * cbrt(STEP_SECONDS) // fetch_fail
      if(last_commit_time == 0) {
          return old_pri + step  // 没进行过提交
      }
      
      interval = nowTime - last_commit_time
      if(interval <= 0) {
          
          return old_pri + step
      }
      return old_pri + cbrt(interval)
  }
  ```
  
  通过这种方式保持最近更新的仓库优先级较高（数值越小优先级越高），通过每次+step防止优先级饥饿问题

#### 	worker

  - repo_urls to poll
  - git_repos[repo_url].push_interval # 历史可统计的，上游软件push的间隔
      => log or sqrt (git_repos[repo_url].push_interval)【pri累加的方法能保持平均和公平】
  - github/gitee rate limit # 常数【?】
=>
  - git_repos[repo_url].poll_interval = A * B / C




  ## 故障检测和自动恢复机制

  crash, disk down, network down 

 1、 coordinator hangup

  

  2、 worker hangup

  一个worker挂了，根据【hash机制】coordinator将当前此worker上维护的repo下发给其他worker（分配较为平均）。此时只需要将其他元数据置0，pri置为max（从pri_queue中获取max pri）

  

  3、Aggregator hangup

  

  ...

  

# 数据结构

- 配置文件格式
- 磁盘repos dir layout
- API/消息格式
- 内部数据结构



## 数据存储

### Coordinator

git clone to disk -> read into memory datastruct => git_tree

1、manage git_urls

> **a git tree to store git urls - [in memory]**
>
> git_urls/
> dir/files 1000 urls per file
>
>    - git_url:
>      [key: val] 
>    workers: [worker_id1, worker_id2]
>    ...
>    - git_url:

2、manage workers	

```json
{
	worker_id:  // use 【bitmap】 to alloc
    socket_fd:  // map with worker_id
	pub_ip:
	pub_port:
	role: worker/aggregator  // also includes aggregator
	alive:
	last_alive_time:
}
```



### Worker

自己维护repo的元数据（需要存一份本地日志文件）（主要为了优先级pri服务，不需要持久化）

一个worker挂了，coordinator将当前此worker上维护的repo下发给其他worker（较为平均），此时只需要将其他元数据置0，pri置为max（从pri_queue中获取max pri）

**元数据**

=> coordinator_ip_addr (read config file )

=> git_repos_map[repo_url]

```
struct repo_url {
    push_interval:
    clone_fail_cnt:
    fetch_fail_cnt:
    queue: true/false  // 判断是否在任务队列中【?】
    ...
}
```

=> git_queue 任务队列

=> priority_queue 优先级队列



  ## Config_file

  ...

 

  

  ## 启动流程

  - #### coordinator

cood_config_file:
	listen port
	git_urls git repo url

local disk space:
	git_urls git repo

startup:
    read cood_config_file -> ( git clone/fetch/update urls_git_repo ) 
    load all git_urls git content to memory、create data structure （buf）in memory

event loop:
	on worker register:
	on worker register its local urls:
	on worker tell one url new status 404/new_commit:
	new thread-> git fetch urls_git_repo on time：

func:
	monitor_git_urls_git on new thread:

```ruby
        loop:
            fetch_info = %x(git -C #{urls_git_repo} fetch)
            if fetch_info.include('->') # have new commit
	    for added repos:
	        assign worker
                # 如何知道哪部分增量添加了？=> git log, check +/- lines
                update_database(); 
	    for deleted repos:
	        tell worker to del
                update_database(); 
```

listen

new thread【?】-> send keepalive to worker（on time）




scheduler algorithm:
- avoid thrashing (worker1 online => 1 minute/hour later => worker2 oneline => )
  - tell worker1 to remove work list in time
- on worker online:
  - don't tell worker1 all urls at once, one batch at a time
- on worker offline
  

re-assign according to new consistent hash  

not prefer: if some old workers have local cache, can tell them to poll for now (complex, not by consistent hash)

 

 - #### worker

config file:
	where is coordinator
	disk path

git repos disk layout:
  	raw local git repos

in-memory data:
  	git urls that have raw local git repos

startup:
    connect to coordinator
    scan per-url git dir, get local_urls list
    tell coordinator

event loop:
    on add new url request: get from coordinator new urls to process
    on del old url request: 
    on fetch old url request: do local git fetch
	on ping request: receive keepalive request -> ack to coordinator

func:
	fetch_repo:
		if found git url 404
				API tell coordinator
		if network error
				arrange local retries
		if disk error
				API tell coordinator I'm down




 - #### aggregator
    scan per-url git dir, get local_urls list &
    connect to coordinator
    report local_urls list to coordinator

  

  

  ## **功能列表：**

  **Story 1: Pull Upstream Repo**

  **Story 2: GitHub Webhook**

  - 功能：通过GitHub的Webhook机制实现Git仓库的自动同步。
  - 包含的操作：
    - 注册GitHub Webhook，将仓库的更新事件推送到系统中。
    - 根据接收到的Webhook消息，识别出哪个Git仓库有更新。
    - 触发相应的Worker执行Git Pull操作，拉取更新的数据。
  - 输入：GitHub Webhook消息（包含更新的Git仓库URL等信息）
  - 输出：无（触发Git Pull操作）

  **Story 3: On Demand (API)**

  - 功能：提供API接口，允许外部用户按需触发Git仓库的拉取操作。
  - 包含的操作：
    - 设计API接口，接收外部用户提交的Git仓库URL和拉取请求。
    - 根据API请求，将拉取任务分配给合适的Worker进行处理。
    - Worker执行Git Pull操作，获取最新的仓库数据，并将结果返回给用户。
  - 输入：外部用户提交的拉取任务请求（包含Git仓库URL和其他相关信息）

  - 输出：任务状态和结果数据。

  **Story 4: Dynamic Add/Update Git URLs (API)**

  - 功能：提供API接口，允许动态添加或更新需要拉取的Git仓库URL。
  - 包含的操作：
    - 设计API接口，接收外部用户提交的Git仓库URL和相关信息。
    - 将新的Git仓库URL添加到系统中，或者更新已有的Git仓库URL的相关信息。
    - 根据情况将新增或更新的Git仓库URL分配给合适的Worker进行处理。
  - 输入：外部用户提交的Git仓库URL和相关信息
  - 输出：Git仓库的添加/更新状态信息。 

​    

  需求列表
  DFX列表
  哪些数据 在哪里 怎么传递
  config file form?

  1M urls store in a git tree
  1 file store 1000 url?
  1k files, each file 1k urls, 100kb per file
  100MB data
  only coordinator git clone urls config file 
  disk data, dir layout



## API设计

1. **提交任务API（On Demand）：**

   - 接口：POST /api/tasks

   - 输入：

     ```
     {
       "git_url": "https://github.com/username/repo.git",
       "additional_params": { /* 可选的其他参数 */ }
     }
     ```

   - 输出：

     ```
     {
       "task_id": "123456",
       "status": "pending"
     }
     ```

2. **获取任务状态API：**

   - 接口：GET /api/tasks/{task_id}

   - 输出：

     ```
     {
       "task_id": "123456",
       "status": "running",
       "progress": "50%",
       "result": null /* 或者包含任务结果的数据 */
     }
     ```

3. **动态添加/更新Git URL API：**

   - 接口：POST /api/git_urls

   - 输入：

     ```
     {
       "git_url": "https://github.com/username/repo.git",
       "additional_info": { /* 可选的其他信息 */ }
     }
     ```

   - 输出：

     ```
     {
       "git_url": "https://github.com/username/repo.git",
       "status": "added"
     }
     ```





------

1. Coordinator分配任务：系统预先配置好需要拉取的Git仓库URL列表，并存储在Coordinator中。Coordinator负责将这些Git仓库URL分配给不同的Worker进行处理。
2. Worker更新Git仓库：Worker接收到来自Coordinator的任务分配后，从上游Git托管服务（如GitHub、GitLab、Gitee等）更新指定的Git仓库数据，并实现增量同步。Worker定期轮询上游仓库，根据更新活跃度自适应间隔进行Git Pull操作。
3. Worker汇报状态：Worker执行Git Pull操作后，将本地状态和数据汇报给Coordinator，以便系统管理和任务调度。
4. Coordinator处理任务结果：Coordinator接收到来自Worker的任务执行状态和结果后，可能将结果返回给Aggregator，或者直接处理任务结果，取决于系统的具体设计。
5. Aggregator汇总数据：Aggregator负责从各个Worker节点收集数据，并进行汇总处理。它将所有拉取的仓库数据集中在一起，为后续的用户查询提供统一的数据接口。
6. 用户获取数据：外部用户可以通过Aggregator提供的API接口，根据任务ID或其他查询条件，获取之前拉取的Git仓库数据。
7. 动态管理：系统可以支持动态管理Git仓库URL列表，可以随时添加或更新需要拉取的Git仓库URL。
8. GitHub Webhook支持：在系统配置了GitHub Webhook的情况下，当Git仓库有更新时，GitHub会向Aggregator发送Webhook消息，触发相应的任务调度和Git Pull操作。
